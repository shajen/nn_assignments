{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.ndimage\n",
    "\n",
    "import PIL\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"PIL\").setLevel(logging.INFO)\n",
    "\n",
    "import common.plotting\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import tensorflow\n",
    "\n",
    "import os\n",
    "import ot\n",
    "import itertools\n",
    "import datetime\n",
    "import sys\n",
    "from random import randint\n",
    "from pyriemann.utils.distance import *\n",
    "\n",
    "os.environ['TORCH_MODEL_ZOO'] =  os.environ['PYTORCH_DATA_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_torch_config(filename):\n",
    "    return torch.load('%s/%s' % (DST_CONFIG_DIR, filename))\n",
    "\n",
    "def save_torch_config(data, filename):\n",
    "    if not os.path.exists(DST_CONFIG_DIR):\n",
    "        os.mkdir(DST_CONFIG_DIR)\n",
    "    return torch.save(data, '%s/%s' % (DST_CONFIG_DIR, filename))\n",
    "\n",
    "def load_torch_big_data(filename, samples):\n",
    "    return torch.load('%s/%d/%s' % (DST_BIG_DATA_DIR, samples, filename))\n",
    "\n",
    "def save_torch_big_data(data, filename, samples):\n",
    "    if not os.path.exists(DST_BIG_DATA_DIR):\n",
    "        os.mkdir(DST_BIG_DATA_DIR)\n",
    "    if not os.path.exists('%s/%d' % (DST_BIG_DATA_DIR, samples)):\n",
    "        os.mkdir('%s/%d' % (DST_BIG_DATA_DIR, samples))\n",
    "    return torch.save(data, '%s/%d/%s' % (DST_BIG_DATA_DIR, samples, filename))\n",
    "\n",
    "def to_np(x):\n",
    "    if isinstance(x, Variable):\n",
    "        x = x.data\n",
    "    return x.cpu().numpy()\n",
    "\n",
    "def to_variable(x, **kwargs):\n",
    "    x = torch.from_numpy(x)\n",
    "    if CUDA:\n",
    "        x = x.cuda()\n",
    "    return Variable(x, **kwargs)\n",
    "\n",
    "def log(text):\n",
    "    if not os.path.exists(DST_CONFIG_DIR):\n",
    "        os.mkdir(DST_CONFIG_DIR)\n",
    "    text = '%s | %s' % (datetime.datetime.now(), text)\n",
    "    with open('%s/%s' % (DST_CONFIG_DIR, LOG_FILE), 'a') as file:\n",
    "        file.write(text + '\\n')\n",
    "        file.flush()\n",
    "    print(text)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view((x.size(0), ) + self.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ot\n",
    "\n",
    "def ground_matrix(n):\n",
    "    x = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            x.append([i, j])\n",
    "    x = np.array(x)\n",
    "    M = ot.dist(x, x, 'sqeuclidean')\n",
    "    return M\n",
    "\n",
    "def sqeuclidean_wasserstein_distance(x, y):\n",
    "    x = to_np(x)\n",
    "    y = to_np(y)\n",
    "    M = ground_matrix(x.shape[1])\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    y = y.reshape(y.shape[0], -1)\n",
    "    return torch.Tensor([ot.emd2(x[i], y[i], M) for i in range(0, x.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_mnist(samples):\n",
    "    log('start reading mnist data')\n",
    "    \n",
    "    try:\n",
    "        log('read images, labels, distances from files')\n",
    "        images = load_torch_big_data('images.pt', samples)\n",
    "        labels = load_torch_big_data('labels.pt', samples)\n",
    "        distances = load_torch_big_data('distances.pt', samples)\n",
    "    except Exception as e:\n",
    "        log('error, calucalting new data')\n",
    "        data_path = os.environ.get('PYTORCH_DATA_PATH', '../data')\n",
    "\n",
    "        dataset = torchvision.datasets.MNIST(data_path, train=True, download=True)\n",
    "        imagesList = dataset.train_data\n",
    "        imagesList = imagesList.squeeze(1).double()\n",
    "        imagesList = imagesList.div(imagesList.sum(1).sum(1).unsqueeze(1).unsqueeze(1))\n",
    "        labelsList = dataset.train_labels\n",
    "\n",
    "        indexes = set()\n",
    "        while len(indexes) < samples:\n",
    "            indexes.add((randint(0, imagesList.size(0)-1), randint(0, imagesList.size(0)-1)))\n",
    "        indexes = [j for i in list(indexes) for j in i]\n",
    "\n",
    "        images = torch.index_select(imagesList, 0, torch.LongTensor(indexes))\n",
    "        labels = torch.index_select(labelsList, 0, torch.LongTensor(indexes))\n",
    "        images = images.view(-1, 2, images.size(1), images.size(2))\n",
    "        labels = labels.view(-1, 2)\n",
    "        distances = sqeuclidean_wasserstein_distance(images[:, 0], images[:, 1])\n",
    "\n",
    "        images = images.float()\n",
    "        distances = distances.float()\n",
    "\n",
    "        save_torch_big_data(images, 'images.pt', samples)\n",
    "        save_torch_big_data(labels, 'labels.pt', samples)\n",
    "        save_torch_big_data(distances, 'distances.pt', samples)\n",
    "\n",
    "    log('%d samples count' % images.size(0))\n",
    "    log('distances sum: %.2f' % distances.sum())\n",
    "    log('distances min: %.2f' % distances.min())\n",
    "    log('distances max: %.2f' % distances.max())\n",
    "\n",
    "    n_train = int(images.size(0) * 0.7)\n",
    "    n_valid = int(images.size(0) * 0.9)\n",
    "    train_images = images[:n_train]\n",
    "    train_labels = labels[:n_train]\n",
    "    train_distances = distances[:n_train]\n",
    "\n",
    "    valid_images = images[n_train:n_valid]\n",
    "    valid_labels = labels[n_train:n_valid]\n",
    "    valid_distances = distances[n_train:n_valid]\n",
    "\n",
    "    test_images = images[n_valid:]\n",
    "    test_labels = labels[n_valid:]\n",
    "    test_distances = distances[n_valid:]\n",
    "\n",
    "    log('finish loading')\n",
    "    return (train_images, train_labels, train_distances, valid_images, valid_labels, valid_distances, test_images, test_labels, test_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randIndex(classes):\n",
    "    c = randint(0, len(classes) - 1)\n",
    "    s = sum(x.size(0) for x in classes[:c])\n",
    "    i = randint(0, classes[c].size(0) - 1)\n",
    "    return s + i\n",
    "    \n",
    "def load_doodle(samples):\n",
    "    log('start reading doodle data')\n",
    "    \n",
    "    try:\n",
    "        log('read images, labels, distances from files')\n",
    "        images = load_torch_big_data('images.pt', samples)\n",
    "        labels = load_torch_big_data('labels.pt', samples)\n",
    "        distances = load_torch_big_data('distances.pt', samples)\n",
    "    except Exception as e:\n",
    "        log('error, calucalting new data')\n",
    "        data_path = os.environ.get('PYTORCH_DATA_PATH', '../data')\n",
    "\n",
    "        catsImages = torch.from_numpy(np.load('/home/i233123/nn/nn_assignments/project/doodle/cat.npy')).view(-1, 28, 28)\n",
    "        crabsImages = torch.from_numpy(np.load('/home/i233123/nn/nn_assignments/project/doodle/crab.npy')).view(-1, 28, 28)\n",
    "        facesImages = torch.from_numpy(np.load('/home/i233123/nn/nn_assignments/project/doodle/face.npy')).view(-1, 28, 28)\n",
    "        mnistImages = torchvision.datasets.MNIST(data_path, train=True, download=True).train_data\n",
    "\n",
    "        catsImages = catsImages.double()\n",
    "        crabsImages = crabsImages.double()\n",
    "        facesImages = facesImages.double()\n",
    "        mnistImages = mnistImages.squeeze(1).double()\n",
    "        \n",
    "        catsImages = catsImages.div(catsImages.sum(1).sum(1).unsqueeze(1).unsqueeze(1))\n",
    "        crabsImages = crabsImages.div(crabsImages.sum(1).sum(1).unsqueeze(1).unsqueeze(1))\n",
    "        facesImages = facesImages.div(facesImages.sum(1).sum(1).unsqueeze(1).unsqueeze(1))\n",
    "        mnistImages = mnistImages.div(mnistImages.sum(1).sum(1).unsqueeze(1).unsqueeze(1))               \n",
    "        \n",
    "        imagesList = torch.cat((mnistImages, catsImages, crabsImages, facesImages), 0)\n",
    "        labelsList = [0] * catsImages.size(0) + [1] * crabsImages.size(0) + [2] * facesImages.size(0) + [3] * mnistImages.size(0)\n",
    "        labelsList = torch.Tensor(labelsList).int()\n",
    "               \n",
    "        indexes = set()\n",
    "        while len(indexes) < samples:\n",
    "            #i1 = randint(0, imagesList.size(0) - 1)\n",
    "            #i2 = randint(0, imagesList.size(0) - 1)\n",
    "            i1 = randIndex([catsImages, crabsImages, facesImages, mnistImages])\n",
    "            i2 = randIndex([catsImages, crabsImages, facesImages, mnistImages])\n",
    "            indexes.add((i1, i2))\n",
    "        indexes = [j for i in list(indexes) for j in i]\n",
    "\n",
    "        images = torch.index_select(imagesList, 0, torch.LongTensor(indexes))\n",
    "        labels = torch.index_select(labelsList, 0, torch.LongTensor(indexes))\n",
    "        images = images.view(-1, 2, images.size(1), images.size(2))\n",
    "        labels = labels.view(-1, 2)\n",
    "        distances = sqeuclidean_wasserstein_distance(images[:, 0], images[:, 1])\n",
    "\n",
    "        images = images.float()\n",
    "        distances = distances.float()\n",
    "\n",
    "        save_torch_big_data(images, 'images.pt', samples)\n",
    "        save_torch_big_data(labels, 'labels.pt', samples)\n",
    "        save_torch_big_data(distances, 'distances.pt', samples)\n",
    "        \n",
    "    log('%d samples count' % images.size(0))\n",
    "    log('distances sum: %.2f' % distances.sum())\n",
    "    log('distances min: %.2f' % distances.min())\n",
    "    log('distances max: %.2f' % distances.max())\n",
    "\n",
    "    n_train = int(images.size(0) * 0.7)\n",
    "    n_valid = int(images.size(0) * 0.9)\n",
    "    train_images = images[:n_train]\n",
    "    train_labels = labels[:n_train]\n",
    "    train_distances = distances[:n_train]\n",
    "\n",
    "    valid_images = images[n_train:n_valid]\n",
    "    valid_labels = labels[n_train:n_valid]\n",
    "    valid_distances = distances[n_train:n_valid]\n",
    "\n",
    "    test_images = images[n_valid:]\n",
    "    test_labels = labels[n_valid:]\n",
    "    test_distances = distances[n_valid:]\n",
    "\n",
    "    log('finish loading')\n",
    "    return (train_images, train_labels, train_distances, valid_images, valid_labels, valid_distances, test_images, test_labels, test_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printMatrixDistancesLabels(distances, labels, classesCount):\n",
    "    samples = distances.size(0)\n",
    "    distances_matrix_sum = np.zeros((classesCount, classesCount))\n",
    "    distances_matrix_count = np.zeros((classesCount, classesCount))\n",
    "\n",
    "    for i in range(samples):\n",
    "        l1 = max(labels[i])\n",
    "        l2 = min(labels[i])\n",
    "        distances_matrix_sum[l1][l2] += 2\n",
    "        distances_matrix_sum[l1][l2] += distances[i]\n",
    "        distances_matrix_count[l1][l2] += 1\n",
    "\n",
    "    np.set_printoptions(precision=2)\n",
    "    log('\\n' + str(distances_matrix_sum))\n",
    "    log('\\n' + str(distances_matrix_count))\n",
    "    log('\\n' + str(distances_matrix_sum / distances_matrix_count))\n",
    "\n",
    "#     from common.plotting import plot_mat\n",
    "#     for i in range(10):\n",
    "#         d = sqeuclidean_wasserstein_distance(images[i][0].unsqueeze(0), images[i][1].unsqueeze(0))[0]\n",
    "#         lol = to_np(images[i])\n",
    "#         lol = np.array([lol[0], lol[1]])\n",
    "#         lol = np.expand_dims(lol, axis=1)\n",
    "#         plot_mat(lol, cmap='gray')\n",
    "#         plt.title(\"Distance: %.2f\" % d)\n",
    "#         show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error_rate(model, allX, allY, batch_size):\n",
    "    i = 0\n",
    "    mse = 0.0\n",
    "    while i < allX.size(0):\n",
    "        x = Variable(allX[i:i+batch_size])\n",
    "        y = Variable(allY[i:i+batch_size])\n",
    "        if CUDA:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        (outputs, _) = model(x)\n",
    "        diff = (outputs - y).data\n",
    "        mse = mse + torch.sum(diff ** 2)\n",
    "        i = i + batch_size\n",
    "    return mse / allX.size(0)\n",
    "\n",
    "def compute_prediction(model, allX, batch_size):\n",
    "    i = 0\n",
    "    y = torch.FloatTensor()\n",
    "    if CUDA:\n",
    "        y = y.cuda()\n",
    "    while i < allX.size(0):\n",
    "        x = Variable(allX[i:i+batch_size])\n",
    "        if CUDA:\n",
    "            x = x.cuda()\n",
    "        y = torch.cat((y, model(x)[0].data), 0)\n",
    "        i += batch_size\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(1, 20, kernel_size=3, padding=0),\n",
    "                nn.ReLU()),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(20, 10, kernel_size=3, padding=0),\n",
    "                nn.ReLU()),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(10, 5, kernel_size=5, padding=0),\n",
    "                nn.ReLU()),\n",
    "            Reshape(-1),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(2000, 100),\n",
    "                nn.ReLU()),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(100, 50),\n",
    "                nn.ReLU())\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(50, 100),\n",
    "                nn.ReLU()),\n",
    "            nn.Sequential(\n",
    "                nn.Linear(100, 5 * 32 * 32),\n",
    "                nn.ReLU()),\n",
    "            Reshape(5, 32, 32),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(5, 10, kernel_size=1, padding=0),\n",
    "                nn.ReLU()),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(10, 20, kernel_size=3, padding=0),\n",
    "                nn.ReLU()),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(20, 1, kernel_size=3, padding=0),\n",
    "                nn.ReLU()),\n",
    "            nn.Softmax2d()\n",
    "        )\n",
    "        \n",
    "    def forward(self, data):\n",
    "        data1 = data[:, 0].unsqueeze(1)\n",
    "        encoder1 = self.encoder(data1)\n",
    "        decoder1 = self.decoder(encoder1)\n",
    "        \n",
    "        data2 = data[:, 1].unsqueeze(1)\n",
    "        encoder2 = self.encoder(data2)\n",
    "        decoder2 = self.decoder(encoder2)\n",
    "            \n",
    "        encoder_difference = encoder1 - encoder2\n",
    "        encoder_factor = torch.torch.matmul(encoder_difference, encoder_difference.transpose(0, 1)).diag()\n",
    "        \n",
    "        return (encoder_factor, torch.cat((decoder1, decoder2)))\n",
    "      \n",
    "def loadModel(model):\n",
    "    try:\n",
    "        log('load model')\n",
    "        model.load_state_dict(load_torch_config('cnn.pkl'))\n",
    "        log('load model finished')\n",
    "    except:\n",
    "        log('load model error')\n",
    "\n",
    "def trainModel(model, train_images, train_distances, valid_images, valid_distances, learning_rate, samples, batch_size, restore):\n",
    "    log('start training')\n",
    "\n",
    "    epoch = 0\n",
    "    num_epochs = 1\n",
    "    patience_expansion = 1.5\n",
    "    best_value_error = 1000000.0\n",
    "    best_params = None\n",
    "\n",
    "    mse_criterion = nn.MSELoss()\n",
    "    kl_criterion = nn.KLDivLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if restore:\n",
    "        try:\n",
    "            log('restore training data')\n",
    "            model.load_state_dict(load_torch_config('cnn.pkl'))\n",
    "            optimizer.load_state_dict(load_torch_config('optimizer.pkl'))\n",
    "            e = load_torch_config('epoch.pkl')\n",
    "            epoch = e[0]\n",
    "            num_epochs = e[1]\n",
    "            log('restore training data finished')\n",
    "        except:\n",
    "            log('restore training data error')\n",
    "    \n",
    "    try:\n",
    "        while epoch < num_epochs:\n",
    "            epoch += 1\n",
    "            i = 0\n",
    "            while i < train_images.size(0):\n",
    "                optimizer.zero_grad()\n",
    "                x = Variable(train_images[i:i+batch_size])\n",
    "                y = Variable(train_distances[i:i+batch_size])\n",
    "                if CUDA:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "                (distances_outputs, images_outputs) = model(x)\n",
    "                loss1 = mse_criterion(distances_outputs, y)\n",
    "                loss2 = kl_criterion(images_outputs, x)\n",
    "                loss = loss1 + loss2\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                i = i + batch_size\n",
    "\n",
    "            value_error = compute_error_rate(model, valid_images, valid_distances, batch_size)\n",
    "            if (value_error < best_value_error):\n",
    "                best_value_error = value_error\n",
    "                num_epochs = int(np.maximum(num_epochs, epoch * patience_expansion + 1))\n",
    "                best_params = [p.clone().cpu() for p in model.parameters()]\n",
    "\n",
    "            log('epoch [%d/%d]' % (epoch, num_epochs))\n",
    "            log('validation set errors: %.2f' % value_error)\n",
    "            log('')\n",
    "\n",
    "            save_torch_config(model.state_dict(), 'cnn.pkl')\n",
    "            save_torch_config(optimizer.state_dict(), 'optimizer.pkl')\n",
    "            save_torch_config(torch.Tensor([epoch, num_epochs]), 'epoch.pkl')\n",
    "\n",
    "            with open('%s/%s' % (DST_CONFIG_DIR, CONFIG_FILE), 'w') as file:\n",
    "                file.write(\"samples            %d\\n\" % (samples))\n",
    "                file.write(\"num_epochs         %d\\n\" % (num_epochs))\n",
    "                file.write(\"epoch              %d\\n\" % (epoch))\n",
    "                file.write(\"batch_size         %d\\n\" % (batch_size))\n",
    "                file.write(\"patience_expansion %.4f\\n\" % (patience_expansion))\n",
    "                file.write(\"best_value_error   %.4f\\n\" % (best_value_error))\n",
    "                file.write(\"learning_rate      %.8f\\n\" % (learning_rate))\n",
    "                file.flush()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    if best_params is not None:\n",
    "        model.parameters = best_params\n",
    "        \n",
    "    save_torch_config(model.state_dict(), 'cnn.pkl')\n",
    "    save_torch_config(optimizer.state_dict(), 'optimizer.pkl')    \n",
    "\n",
    "    log('finish training')\n",
    "    \n",
    "def plotScatter(title, model, x, y, batch_size):\n",
    "    exact = np.arange(0.0, 45.0, step=0.1)\n",
    "    error = compute_error_rate(model, x, y, batch_size)\n",
    "    log('MSE = %.2f' % error)\n",
    "    _, sorted_indices = torch.sort(y)\n",
    "    y1 = to_np(torch.index_select(y, 0, sorted_indices))\n",
    "    y2 = to_np(torch.index_select(compute_prediction(model, x, batch_size).cpu(), 0, sorted_indices))\n",
    "\n",
    "    mean1 = np.array([])\n",
    "    mean2 = np.array([])\n",
    "    p10_1 = np.array([])\n",
    "    p10_2 = np.array([])\n",
    "    p90_1 = np.array([])\n",
    "    p90_2 = np.array([])\n",
    "    \n",
    "    for x in np.arange(1.25, 46.25, 2.5):\n",
    "        ind = (x - 1.25 < y1) & (y1 < x + 1.25)\n",
    "        diffY = y1[ind] - y2[ind]\n",
    "        if diffY.shape[0] == 0:\n",
    "            continue\n",
    "        error_value = (diffY ** 2).mean()\n",
    "        mean1 = np.append(mean1, x)\n",
    "        mean2 = np.append(mean2, x - error_value)\n",
    "        \n",
    "        p10_1 = np.append(p10_1, x)\n",
    "        p10_2 = np.append(p10_2, x - np.percentile(diffY, 10))\n",
    "        \n",
    "        p90_1 = np.append(p90_1, x)\n",
    "        p90_2 = np.append(p90_2, x - np.percentile(diffY, 90))\n",
    "        \n",
    "    fig = plt.figure(figsize = (10, 8))\n",
    "    plt.plot(exact, exact, '-', label = 'Exact prediction')\n",
    "    plt.plot(mean1, mean2, '-x', label = 'Mean prediction')\n",
    "    plt.plot(p10_1, p10_2, '-x', label = '10th percentile')\n",
    "    plt.plot(p90_1, p90_2, '-x', label = '90th percentile')\n",
    "    plt.ylabel('Predicted Wass. distance')\n",
    "    plt.xlabel('True Wass. distance')\n",
    "    plt.title(title + 'MSE=%.2f' % error)\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = True\n",
    "DST_CONFIG_DIR = 'mnist'\n",
    "DST_BIG_DATA_DIR = '/pio/scratch/1/i233123/data_mnist'\n",
    "LOG_FILE = 'stdout.txt'\n",
    "CONFIG_FILE = 'config.txt'\n",
    "\n",
    "# if os.path.exists(LOG_FILE):\n",
    "#     os.remove(LOG_FILE)\n",
    "\n",
    "# if not os.path.exists('/pio/scratch/1/i233123/'):\n",
    "#     os.mkdir('/pio/scratch/1/i233123/')\n",
    "        \n",
    "def run_mnist(samples, learning_rate, batch_size, restore, train):\n",
    "    global DST_CONFIG_DIR\n",
    "    DST_CONFIG_DIR = ('%s/%.6f' % (DST_CONFIG_DIR, learning_rate)).replace(\".\", \"#\")\n",
    "    (tr_i, tr_l, tr_d, v_i, v_l, v_d, ts_i, ts_l, ts_d) = load_mnist(samples)\n",
    "    printMatrixDistancesLabels(tr_d, tr_l, 10)\n",
    "    \n",
    "    model = CNN()\n",
    "    if CUDA:\n",
    "        model.cuda()\n",
    "\n",
    "    if restore:\n",
    "        loadModel(model)\n",
    "    if train:\n",
    "        trainModel(model, tr_i, tr_d, v_i, v_d, learning_rate, samples, batch_size, restore)\n",
    "    model.eval()\n",
    "    \n",
    "    plotScatter('MNIST test set ', model, ts_i, ts_d, batch_size)\n",
    "    \n",
    "run_mnist(1000000, 0.0005, 1000, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = True\n",
    "DST_CONFIG_DIR = 'doodle'\n",
    "DST_BIG_DATA_DIR = '/pio/scratch/1/i233123/data_doodle'\n",
    "LOG_FILE = 'stdout.txt'\n",
    "CONFIG_FILE = 'config.txt'\n",
    "\n",
    "# if os.path.exists(LOG_FILE):\n",
    "#     os.remove(LOG_FILE)\n",
    "\n",
    "# if not os.path.exists('/pio/scratch/1/i233123/'):\n",
    "#     os.mkdir('/pio/scratch/1/i233123/')\n",
    "        \n",
    "def run_doodle(samples, learning_rate, batch_size, restore, train):\n",
    "    global DST_CONFIG_DIR\n",
    "    DST_CONFIG_DIR = ('%s/%.6f' % (DST_CONFIG_DIR, learning_rate)).replace(\".\", \"#\")\n",
    "    (tr_i, tr_l, tr_d, v_i, v_l, v_d, ts_i, ts_l, ts_d) = load_doodle(samples)\n",
    "    printMatrixDistancesLabels(tr_d, tr_l, 4)\n",
    "        \n",
    "    model = CNN()\n",
    "    if CUDA:\n",
    "        model.cuda()\n",
    "\n",
    "    if restore:\n",
    "        loadModel(model)\n",
    "    if train:\n",
    "        trainModel(model, tr_i, tr_d, v_i, v_d, learning_rate, samples, batch_size, restore)\n",
    "    model.eval()\n",
    "    \n",
    "    plotScatter('Doodle test set ', model, ts_i, ts_d, batch_size)\n",
    "    \n",
    "run_doodle(1000000, 0.0005, 1000, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
